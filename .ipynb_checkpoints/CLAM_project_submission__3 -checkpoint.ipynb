{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1a6b1-c47b-4af2-844a-5f23d76ac3cb",
   "metadata": {
    "id": "b4a1a6b1-c47b-4af2-844a-5f23d76ac3cb"
   },
   "source": [
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th><h1>CS 5364</h1><h2>Information Retrieval</h2>\n",
    "        <h1 style=\"color:maroon;\">Team Project Assignment</h1>\n",
    "        <h2 style=\"color:maroon;\">Third Submission</h2></th>\n",
    "        <th><img src=\"https://www.ttu.edu/traditions/images/raiderstatue.jpg\" width=225 height=116 /></th>\n",
    "        <th><p>Texas Tech University Matador Song</p>\n",
    "            <p>Fight, Matadors, for Tech!<br>\n",
    "                Songs of love we'll sing to thee,<br>\n",
    "                Bear our banners far and wide.<br>\n",
    "                Ever to be our pride,<br>\n",
    "                Fearless champions ever be.<br>\n",
    "                Stand on heights of victory.<br>\n",
    "                Strive for honor evermore.<br>\n",
    "                Long live the Matadors!</p>\n",
    "                <p>Music by Harry Lemaire, words by R.C. Marshall</p></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7f8fb-882d-4098-adea-e82a20733aa5",
   "metadata": {
    "id": "4fb7f8fb-882d-4098-adea-e82a20733aa5"
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">Enter the Team Name Here</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Steve Mwika</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Mallikarjuna Reddy Bobbala</bf></h4></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507e5be-5fe6-415b-afa0-ed6e1f22f27c",
   "metadata": {
    "id": "f507e5be-5fe6-415b-afa0-ed6e1f22f27c"
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">IR Problem</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>What is the problem on which the team worked for this submission?</bf></h4></li>\n",
    "    <ul>\n",
    "        <li>The problem we propose to address is Fake News Detection Based on Natural Language Processing (NLP) and Generative Pre-trained Transformer (GPT).</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Is it the same or a different problem than the problem(s) proposed in the proposal or the second submission?  If changes in scope or other changes to the problem have been made since the proposal or second submission, please explain here.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li>The same problem as proposed in the proposal.</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Please summarize here what advances and lessons learned the team made in solving the problem for this submission.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Advances:</h5>\n",
    "            <ul>\n",
    "                <li>Model Optimization: The squad's ability to manage a variety of architectures and tailor them for certain tasks is demonstrated by the successful completion of training both GPT and BERT-based NLP models.</li>\n",
    "                <li>Experimental Validation: A comprehensive analysis of the experimental findings produced improved understanding of the accuracy, robustness, and performance of the model.</li>\n",
    "                 <li>Collaboration and Documentation: When it came to reporting tasks, team members worked together to provide clear communication and thorough documentation of results, which improved the project deliverables' overall quality.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><h5>Lessons Learned:</h5>\n",
    "            <ul>\n",
    "                <li>Training Efficiency: To finish large-scale model training procedures on time, it was essential to optimize training schedules and computing resources.\n",
    "</li>\n",
    "                <li>Evaluation Importance: An organized assessment process revealed model flaws and emphasized the necessity of iterative testing and enhancements.</li>\n",
    "                <li>Teamwork and Time Management: Clear roles and efficient team communication were emphasized via cooperative efforts in report preparation.</li>\n",
    "                <li>Testing Procedures: The testing stage emphasized how crucial it is to select suitable metrics and situations in order to evaluate the practicality of trained models.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939f91c-d08c-43f1-8290-9f6e0507e92a",
   "metadata": {
    "id": "1939f91c-d08c-43f1-8290-9f6e0507e92a"
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">What other ideas, models, approaches, and/or helpful suggestions have you found since the last team submission related to solving the IR problem or similar problems by others?</h1>\n",
    "\n",
    "<hr>\n",
    "<h2 style=\"color: teal\">From Journal/Conference Papers</h2>\n",
    "\n",
    "<p></p>\n",
    "<p></p>\n",
    "\n",
    "<p><a href=</a>  <a href\"></a> </p>\n",
    "<ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>E. Hashmi, S. Y. Yayilgan, M. M. Yamin, S. Ali and M. Abomhara, \"Advancing Fake News Detection: Hybrid Deep Learning With FastText and Explainable AI,\" in IEEE Access, vol. 12, pp. 44462-44480, 2024, doi: 10.1109/ACCESS.2024.3381038.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  Public trust, societal stability, and the integrity of information systems are the one seriously threatened by the spreading of false information and the fake news on digital platforms, especially social media. This is the problem calls for the creation of reliable detection techniques that can distinguish between fake and legitimate news while taking scalability and linguistic complexity into account.</li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  1. Methods Based on Machine Learning:\n",
    "Methods have been explored that are based on features such as TF-IDF or n-grams, including Naive Bayes, Support Vector Machines (SVM), Decision Trees, and Random Forests. For scalability, frameworks like Spark have also been utilized.\n",
    "2. Deep Learning Techniques: In order to handle sequential data, models such as CNNs, GRUs, and LSTMs have been used.\n",
    "Improved syntactic and semantic understanding is facilitated by transformer models such as BERT, RoBERTa, and XLNet.\n",
    "</li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> Hybrid Model: This paper has proposed the hybrid model which combines the CNN-LSTM enriched with the Fastext that has achieved greater accuracy, This paper also has proposed enhanced transformer based architectires like BERT applied for advanced contextual capabilities</li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  1. Multilingual Fake News Detection: By Using multilingual transformers such as mBERT and mT5 to widen fake news detection to various languages.\n",
    "2. Adversarial Training: Improve resilience to evolving disinformation strategies.\n",
    "3. Adding More Context: To increase the precision of identifying false information in media-rich content, include data that is multimodal, such as videos and pictures.\n",
    "</li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  1. measures: We can use the computational efficiency parameters like accuracy, precision, recall, F1 score to assess the model\n",
    "2. The capacity to explain Testing: We can use LIME and LDA to assess models decision making transparency\n",
    "3. Dataset Variability: By testing the models across diverse datasets in order to ensure the generalizability\n",
    "g</li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  1. Hybrid Detection System: We can try to adopt the hybrid approach of CNN_LSTM combined for robust handling of textual nuances\n",
    "2. Transformer-Based Insights: Leveraging the transformer models like BERT for more semantic understanding as their results are highly reliable\n",
    "</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>S. F. N. Azizah, H. D. Cahyono, S. W. Sihwi and W. Widiarto, \"Performance Analysis of Transformer Based Models (BERT, ALBERT, and RoBERTa) in Fake News Detection,\" 2023 6th International Conference on Information and Communications Technology (ICOIACT), Yogyakarta, Indonesia, 2023, pp. 425-430, doi: 10.1109/ICOIACT59844.2023.10455849.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5> The growing number of false news presents significant challenges for society as it deceives people and groups, incites conflict, and advances particular agendas. It can be difficult to distinguish between real and fake material because of time restrictions, a lack of subject-matter expertise, and the intricacy of disinformation tactics. </li>\n",
    "        <li><h5>Past Solution Ideas:</h5> 1. Transformer Models: By utilising their contextual understanding skills, BERT, ALBERT, and RoBERTa have been successfully employed to detect bogus news.\n",
    "BERT: Offers bidirectional text comprehension, but requires a lot of processing power.\n",
    "ALBERT: A condensed form of BERT designed for low-resource devices.\n",
    "RoBERTa: superior performance on NLP tasks and improved training methods over BERT.\n",
    "2. Hybrid Approaches: To improve sequential and semantic text analysis, combine transformers with models such as CNN and Bi-LSTM.\n",
    "3. Preprocessing and Tokenisation: Methods for getting text data ready for model input include stop-word elimination, tokenisation, and word embedding optimisation\n",
    " </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> 1.Using Bahasa Indonesia datasets, a comparative analysis of transformer models (BERT, ALBERT, and RoBERTa) for the detection of fake news.\n",
    "2.Stressing how ALBERT performs better in terms of efficiency and accuracy (87.6%), making it the best option for contexts with limited resources.\n",
    "3. Optimising model input through the use of specialised tokenisation approaches (BertTokenizer, RobertaTokenizer).\n",
    " </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>1.\tAdvanced Tokenization: Experiment with new tokenizers to enhance pre-trained transformer models' performance further.\n",
    "2.\tMultilingual Support: Extend fake news detection capabilities to multiple languages beyond Bahasa Indonesia.\n",
    "3.\tLightweight Models: Focus on models like ALBERT that offer scalability and efficiency for broader deployment.\n",
    "4.\tRobust Evaluation: Enhance testing environments to reduce variability caused by hardware constraints.\n",
    "  </li>\n",
    "        <li><h5>Evaluation Ideas:</h5> 1.To evaluate a model's efficacy, use common measures like accuracy, precision, and F1-score.\n",
    "2.To guarantee scalability, assess runtime efficiency (e.g., seconds per epoch).\n",
    "3.To assess generalisability across datasets, use stringent validation techniques like cross-validation.\n",
    " </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>1.\tHybrid Transformer Systems: Combine the text comprehension capabilities of BERT and RoBERT with GPT component to enhance hybrid detection.\n",
    "2.\tCustom Tokenizers: Develop or adapt tokenizers tailored to dataset for improved input representation.\n",
    "  </li>\n",
    "    </ul>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e750733-0c99-440a-9abc-9fa8da8d722d",
   "metadata": {
    "id": "0e750733-0c99-440a-9abc-9fa8da8d722d"
   },
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">From Other Informal Resources</h2>\n",
    "<hr>\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>https://github.com/wutonytt/Fake-News-Detection</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5>Model Fine-Tuning  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5>For different downstream tasks, we need to conduct different fine-tuning approaches.We have the models for different downstream tasks. In our project of fake news detection, which is the classification of statements, we used bertForSequenceClassification to fine-tune our pre-trained BERT model. The modules of the model contain a BERT module handling various embeddings, a BERT transformer encoder, a BertPooler, a dropout layer, and a linear classifier that returns logits of the 2 classes.  </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa84f4-a1dd-4df8-8fb2-25088b18e92e",
   "metadata": {
    "id": "0daa84f4-a1dd-4df8-8fb2-25088b18e92e"
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Past Plans and Actual Tasks for the Third Project Assignment Submission</h1>\n",
    "\n",
    "<p>The third project assignment submission is the final status report on how far the team has gotten in solving the problem.  Overall, status reports will include but not be limited to items, such as updates to the problem scope, lessons learned, finding more ideas in the conference and journal paper literature as well as informal resources, data sources found, current solution status and performance, comparison of solution to past approaches, software developed and packages used, hardware used, testing, and consideration of new solution approaches.</p>\n",
    "\n",
    "<p>Each team member should catalog the actual work tasks performed for this submission including specfic papers and resources found, IR solution models proposed, preliminary investigative work on a software prototype, researching evaluation strategies for the team's solution, finding needed data files, interviewing experts, solving subproblems, planning, designing, coding, testing, and anything related to helping the team complete the submission well.</p>\n",
    "\n",
    "<p>Tasks should have enough detail to understand clearly and specifically what was done.  For example, rather than say, \"found and wrote up 2 conference papers\", include the authors, such as \"found and wrote up 2 conference papers by Smith et al, 2022, and Breugrand et al, 2019\" so it is clear which papers were contributed to a submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Each team member plans to apply an IR solution to the problem and has the following accomplishments and plans:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: maroon\"><bf>Team Member Name</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Planned Team Member Tasks for Third Submission</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Actual Team Member Tasks for Third Submission</bf></th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Steve Mwikae</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Complete the training of NLP model, e.g., BERT-based model, 10/24/2024</li>\n",
    "                <li>Complete the testing phase and get the results, 10/28/2024</li>\n",
    "                <li>Assist teammates with report for the third assignment, 11/11/2024</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Complete the training of NLP model, e.g., BERT-based model, 11/10/2024</li>\n",
    "                <li>Complete the testing phase and get the results, 11/15/2024</li>\n",
    "                <li>Assist teammates with report for the third assignment, 11/25/2024</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Search for research papers related to our project, 11/14/2024</li>\n",
    "                <li>Completed the training of NLP model, e.g., BERT-based model, 11/15/2024</li>\n",
    "                <li>Completed the testing phase and get the results, 11/20/2024</li>\n",
    "                <li>Assist teammates with report for the third assignment, 11/25/2024</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Mallikarjuna Reddy Bobbala</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Search for research papers related to our project, 10/11/2024</li>\n",
    "                <li>Complete the code for data preprocessing, 10/15/2024</li>\n",
    "                <li>Complete data preprocessing, 10/16/2024</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Complete the training of GPT model, 11/10/2024</li>\n",
    "                <li>Complete the evaluation of experimental results, 11/18/2024</li>\n",
    "                <li>Assisted teammate with report for the third assignment, 11/25/2024<</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Search for research papers related to our project, 11/11/2024</li>\n",
    "                <li>Completed the training of GPT model, 11/16/2024</li>\n",
    "                <li>Completed the evaluation of experimental results, 11/21/2024</li>\n",
    "                <li>Assisted teammate with report for the third assignment, 11/25/2024</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f71f4-6357-4057-8c39-5d8a6d7b800c",
   "metadata": {
    "id": "c36f71f4-6357-4057-8c39-5d8a6d7b800c"
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Current Solution Status</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffa796-2ef5-46fd-bdf7-1d7774298edb",
   "metadata": {
    "id": "1fffa796-2ef5-46fd-bdf7-1d7774298edb"
   },
   "source": [
    "<h2 style=\"color:teal\">What computer hardware, programming language, main software packages, and data files are recommended to be used to run the software for this submission?</h2>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <h5 style=\"color:maroon\">Computer Hardware</h5>\n",
    "            <ul>\n",
    "                <li>Processor: AMD Ryzen 5 5600 6-Core Processor, 3501 Mhz, 6 Core(s), 12 Logical Processor(s)</li>\n",
    "                <li>RAM: 32GB</li>\n",
    "                <li>System type: 64-bit operating system, x64-based processor</li>\n",
    "                <li>GPU: RTX 3060 Ti</li>\n",
    "                <li>SSD: 500 GB, External Drive: 1 TB</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><h5 style=\"color:maroon\">Programming Language</h5>Python 3.12.7</li>\n",
    "        <li><h5 style=\"color:maroon\">Main Software Packages</h5>os, torch, pandas, numpy, wordcloud, transformers, sklearn, datasets, matplotlib.pyplot, seaborn, PIL, re, string, nltk, datetime, time.</li>\n",
    "        <li><h5 style=\"color:maroon\">Data</h5>ISOT Fake News Dataset, BuzzFeed News Analysis and Classification, PolitiFact_fake_news_content, PolitiFact_real_news_content, fake_or_real_news</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b49be-2cd0-4046-9da7-b6dac2274235",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">What is the performance of the team's solutions in comparison with results from journal/conference papers and other informal resources?</h2>\n",
    "\n",
    "<p>For the third submission, each team member has provided performance metrics for individual models trained on the dataset. Below is a detailed comparison of the models: DistilBERT, ELECTRA, and OPT.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal;\">Team Solution 1: Individual Models for Fake News Detection</h2>\n",
    "<div style=\"border: 2px solid teal; border-radius: 10px; padding: 20px; background-color: #f9f9f9;\">\n",
    "    <h2 style=\"color: teal; text-align: center;\">Performance Evaluation of Individual Models</h2>\n",
    "    <p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "        This section provides a comprehensive analysis of the performance of each model trained for fake news detection. \n",
    "        The evaluated models have been carefully selected for their unique strengths in tackling text classification tasks \n",
    "        and optimized to achieve high accuracy and efficiency. Each model has undergone rigorous training and testing to \n",
    "        ensure reliable and reproducible results.\n",
    "    </p>\n",
    "    <div style=\"display: flex; justify-content: space-between; gap: 20px; flex-wrap: wrap;\">\n",
    "        <div style=\"flex: 1; border: 1px solid teal; border-radius: 10px; padding: 10px; background-color: #ffffff;\">\n",
    "            <h3 style=\"color: teal; text-align: center;\">DistilBERT</h3>\n",
    "            <p style=\"text-align: justify; font-size: 0.95em;\">\n",
    "                DistilBERT is a lightweight transformer model that retains 97% of BERT's performance \n",
    "                while being 60% smaller. It is optimized for efficiency and performs well on text classification tasks \n",
    "                with minimal computational overhead.\n",
    "            </p>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; border: 1px solid teal; border-radius: 10px; padding: 10px; background-color: #ffffff;\">\n",
    "            <h3 style=\"color: teal; text-align: center;\">OPT Classifier</h3>\n",
    "            <p style=\"text-align: justify; font-size: 0.95em;\">\n",
    "                The OPT Classifier leverages the OPT transformer architecture to fine-tune a sequence classification model \n",
    "                specifically for fake news detection. \n",
    "            </p>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; border: 1px solid teal; border-radius: 10px; padding: 10px; background-color: #ffffff;\">\n",
    "            <h3 style=\"color: teal; text-align: center;\">ELECTRA</h3>\n",
    "            <p style=\"text-align: justify; font-size: 0.95em;\">\n",
    "                ELECTRA employs a generator-discriminator framework, providing efficient and accurate text classification \n",
    "                by identifying replaced tokens during training. Its discriminator excels in binary classification tasks.\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h2 style=\"color:maroon;\">DistilBERT</h3>\n",
    "<p><b>Description:</b> DistilBERT is a distilled version of BERT that retains 97% of its performance while being 60% smaller. It was trained on the fake news dataset using a binary classification approach.</p>\n",
    "<h4 style=\"color:darkblue;\">Training Process:</h4>\n",
    "<ul>\n",
    "   <li>Dataset: Preprocessed dataset using lemmatization, stopword removal, and balanced sampling to address class imbalance.</li>\n",
    "   <li>Hyperparameters:\n",
    "      <ul>\n",
    "         <li>Learning rate: 5e-5 (moderate to ensure stable convergence)</li>\n",
    "         <li>Batch size: 16 (gradient accumulation steps = 2)</li>\n",
    "         <li>Epochs: 15 </li>\n",
    "         <li>Warmup steps: 10% of steps per epoch (ensures stable initial training)</li>\n",
    "         <li>Evaluation strategy: Frequent evaluations (~7% of an epoch) for early stopping and monitoring.</li>\n",
    "         <li>lr_scheduler_type: linear</li>\n",
    "      </ul>\n",
    "   </li>\n",
    "   <li>Loss Function: Binary cross-entropy loss for classification tasks.</li>\n",
    "   <li>Callback: Early stopping (patience = 4) to prevent overfitting and conserve resources.</li>\n",
    "</ul>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-start;\">\n",
    "    <!-- Left Column: Evaluation Metrics -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Evaluation Metrics:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Metric</th>\n",
    "                <th style=\"color:darkgreen; text-align:center; border-bottom: 2px solid darkgreen;\">Value</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Accuracy</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">92.6%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">F1 Score</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">92.4%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">ROC AUC Score</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">97.3%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Log Loss</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.2489</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Brier Score Loss</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.0625</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">MCC</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.8500</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Cohen's Kappa</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.8480</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    <!-- Right Column: Visual Results -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Visual Results:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; height: 100%;\">\n",
    "            <tr>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Confusion Matrix</th>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">ROC Curve</th>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Precision-Recall Curve</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/distilbert_only/DistilBERT_confusion_matrix.png\" alt=\"DistilBERT Confusion Matrix\" width=\"100%\">\n",
    "                </td>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/distilbert_only/DistilBERT_roc_curve.png\" alt=\"DistilBERT ROC Curve\" width=\"100%\">\n",
    "                </td>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/distilbert_only/DistilBERT_precision_recall_curve.png\" alt=\"DistilBERT Precision-Recall Curve\" width=\"100%\">\n",
    "                </td>\n",
    "            </tr>\n",
    "            <!-- Add filler rows to match height -->\n",
    "            <tr>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<h3 style=\"color:maroon;\">OPT Classifier</h3>\n",
    "<p><b>Description:</b> The OPT Classifier is a transformer-based model optimized for sequence classification. It was fine-tuned using the fake news dataset.</p>\n",
    "<h4 style=\"color:darkblue;\">Training Process:</h4>\n",
    "<ul>\n",
    "   <li>Dataset: Preprocessed dataset with tokenization and padding.</li>\n",
    "   <li>Hyperparameters:\n",
    "      <ul>\n",
    "         <li>Learning rate: 3e-5 (fine-tuned for stability with larger models)</li>\n",
    "         <li>Batch size: 16 (gradient accumulation steps = 2)</li>\n",
    "         <li>Epochs: 10 </li>\n",
    "         <li>Warmup steps: 10% of steps per epoch (stabilizes learning rate ramp-up)</li>\n",
    "         <li>lr_scheduler_type=\"cosine\"</li>\n",
    "      </ul>\n",
    "   </li>\n",
    "   <li>Loss Function: Binary cross-entropy loss for classification tasks.</li>\n",
    "   <li>Callback: Early stopping (patience = 4) to optimize training efficiency and prevent likel</li>\n",
    "</ul>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-start;\">\n",
    "    <!-- Left Column: Evaluation Metrics -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Evaluation Metrics:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Metric</th>\n",
    "                <th style=\"color:darkgreen; text-align:center; border-bottom: 2px solid darkgreen;\">Value</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Accuracy</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">94.3%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">F1 Score</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">94.3%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">ROC AUC Score</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">98.9%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Log Loss</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.1826</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Brier Score Loss</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.0496</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">MCC</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.8876</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px;\">Cohen's Kappa</td>\n",
    "                <td style=\"text-align:center; padding: 5px;\">0.8867</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    <!-- Right Column: Visual Results -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Visual Results:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; height: 100%;\">\n",
    "            <tr>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Confusion Matrix</th>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">ROC Curve</th>\n",
    "                <th style=\"color:darkblue; text-align:center; border-bottom: 2px solid darkblue;\">Precision-Recall Curve</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/opt_only/OPT_confusion_matrix.png\" alt=\"OPT Confusion Matrix\" width=\"100%\">\n",
    "                </td>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/opt_only/OPT_roc_curve.png\" alt=\"OPT ROC Curve\" width=\"100%\">\n",
    "                </td>\n",
    "                <td style=\"text-align:center; padding: 5px; vertical-align: top;\">\n",
    "                    <img src=\"./outputs/opt_only/OPT_precision_recall_curve.png\" alt=\"OPT Precision-Recall Curve\" width=\"100%\">\n",
    "                </td>\n",
    "            </tr>\n",
    "            <!-- Add filler rows to match height -->\n",
    "            <tr>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h3 style=\"color:maroon;\">ELECTRA</h3>\n",
    "<p><b>Description:</b> ELECTRA employs a generator-discriminator framework. The generator replaces tokens in the input with plausible alternatives, and the discriminator predicts whether each token is original or replaced. The discriminator was fine-tuned for binary classification tasks on the fake news dataset.</p>\n",
    "<h4 style=\"color:darkblue;\">Training Process:</h4>\n",
    "<ul>\n",
    "   <li>Dataset: Preprocessed with tokenization, lemmatization, and stopword removal. Balanced sampling was applied to address class imbalance.</li>\n",
    "   <li>Hyperparameters:\n",
    "      <ul>\n",
    "         <li>Learning rate: 5e-5 (adjusted for optimal gradient updates).</li>\n",
    "         <li>Batch size: 16 </li>\n",
    "         <li>Epochs: 10.</li>\n",
    "         <li>Warmup steps: 10% of total steps.</li>\n",
    "         <li>Evaluation strategy: Steps-based evaluation (evaluates every ~7% of an epoch).</li>\n",
    "         <li>lr_scheduler_type: linear .</li>\n",
    "      </ul>\n",
    "   </li>\n",
    "   <li>Loss Function: Binary cross-entropy loss optimized for binary classification.</li>\n",
    "   <li>Callback: Early stopping (patience = 3) was used to halt training upon observing diminishing returns.</li>\n",
    "</ul>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-start;\">\n",
    "    <!-- Left Column: Evaluation Metrics -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Evaluation Metrics:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "               <th style=\"color:darkblue; text-align:center\">Metric</th>\n",
    "               <th style=\"color:darkgreen; text-align:center\">Value</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">Accuracy</td>\n",
    "               <td style=\"text-align:center\">92.1%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">F1 Score</td>\n",
    "               <td style=\"text-align:center\">92.2%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">ROC AUC Score</td>\n",
    "               <td style=\"text-align:center\">97.0%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">Log Loss</td>\n",
    "               <td style=\"text-align:center\">0.2501</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">Brier Score Loss</td>\n",
    "               <td style=\"text-align:center\">0.0647</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">MCC</td>\n",
    "               <td style=\"text-align:center\">0.8460</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "               <td style=\"text-align:center\">Cohen's Kappa</td>\n",
    "               <td style=\"text-align:center\">0.8427\n",
    "        </table>\n",
    "        </div>\n",
    "        <!-- Right Column: Visual Results -->\n",
    "        <div style=\"width: 48%; padding: 10px;\">\n",
    "        <h4 style=\"color:darkblue;\">Visual Results:</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; height: 100%;\">\n",
    "            <tr>\n",
    "              <th style=\"color:darkblue; text-align:center\">Confusion Matrix</th>\n",
    "              <th style=\"color:darkblue; text-align:center\">ROC Curve</th>\n",
    "              <th style=\"color:darkblue; text-align:center\">Precision-Recall Curve</th>\n",
    "           </tr>\n",
    "           <tr>\n",
    "              <td style=\"text-align:center\"><img src=\"./outputs/electra_only/ELECTRA_confusion_matrix.png\" alt=\"ELECTRA Confusion Matrix\" width=\"200\"></td>\n",
    "              <td style=\"text-align:center\"><img src=\"./outputs/electra_only/ELECTRA_roc_curve.png\" alt=\"ELECTRA ROC Curve\" width=\"200\"></td>\n",
    "              <td style=\"text-align:center\"><img src=\"./outputs/electra_only/ELECTRA_precision_recall_curve.png\" alt=\"ELECTRA Precision-Recall Curve\" width=\"200\"></td>\n",
    "           </tr>\n",
    "           <!-- Add filler rows to match height -->\n",
    "            <tr>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "                <td style=\"height: 50px;\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal;\">Team Solution 2: Hybrid Models for Fake News Detection</h2>\n",
    "<div style=\"border: 2px solid teal; border-radius: 10px; padding: 20px; background-color: #f9f9f9;\">\n",
    "   <h2 style=\"color: teal; text-align: center;\">Hypothesis</h2>\n",
    "   <p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "      We hypothesized that combining discriminative models (e.g., DistilBERT, ELECTRA) with generative models \n",
    "      (e.g., GPT-2, OPT-Generator) would enhance fake news detection. The generative component was expected to improve \n",
    "      semantic understanding and contextual reasoning, leading to higher overall accuracy for both real and fake news \n",
    "      classifications.\n",
    "   </p>\n",
    "</div>\n",
    "<div style=\"margin-top: 20px;\">\n",
    "    <h3 style=\"color: maroon; text-align: center;\">Performance Summary of Hybrid Models</h3>\n",
    "    <table style=\"width: 100%; border-collapse: collapse; text-align: center; margin-top: 20px; font-size: 1.1em;\">\n",
    "        <thead>\n",
    "            <tr style=\"background-color: #e0f7fa;\">\n",
    "                <th style=\"border: 1px solid teal; padding: 15px;\">Hybrid Model</th>\n",
    "                <th style=\"border: 1px solid teal; padding: 15px;\">Performance</th>\n",
    "                <th style=\"border: 1px solid teal; padding: 15px;\">Observations</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">DistilBERT + GPT-2</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 43/45<br>\n",
    "                    <b>Fake:</b> 31/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Struggled with fake news; real news detection was solid.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">DistilBERT + OPT-Generator</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 42/45<br>\n",
    "                    <b>Fake:</b> 32/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Slightly better balance than GPT-2 combination but underperformed overall.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">DistilBERT + OPT-Classifier</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 44/45<br>\n",
    "                    <b>Fake:</b> 28/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Overfit to real news, struggled with fake news detection.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">ELECTRA + GPT-2</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 45/45<br>\n",
    "                    <b>Fake:</b> 21/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Excellent real news detection; weak fake news performance.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">ELECTRA + OPT-Generator</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 45/45<br>\n",
    "                    <b>Fake:</b> 1/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Severely biased toward real news detection.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">OPT_Classifier + OPT-Generator</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 42/45<br>\n",
    "                    <b>Fake:</b> 39/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Most balanced performance among all hybrid models.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px; font-weight: bold;\">OPT_Classifier + GPT-2</td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">\n",
    "                    <b>Real:</b> 42/45<br>\n",
    "                    <b>Fake:</b> 39/45\n",
    "                </td>\n",
    "                <td style=\"border: 1px solid teal; padding: 15px;\">Balanced results; good trade-off between both categories.</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 2px solid teal; border-radius: 10px; padding: 20px; background-color: #f9f9f9; margin-top: 20px;\">\n",
    "   <h2 style=\"color: teal; text-align: center;\">Reflection</h2>\n",
    "   <p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "      Despite our hypothesis, the hybrid models did not uniformly improve performance. Some combinations, like \n",
    "      <b>OPT_Classifier + GPT-2</b>, displayed balanced accuracy, but others heavily favored one category over the other, \n",
    "      particularly real news detection. This could stem from suboptimal alignment between discriminative and generative \n",
    "      components or overly aggressive preprocessing. Currently leaning heavily on the side of too much on the agressive \n",
    "      preprocessing. in teh future, Taking a more relaxed approach on hte preprocessing techniwue would help the \n",
    "      Models alot. Stemming and Lemmetazation, I suspect, were massively detrimental in this case.\n",
    "   </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Discussion/Summary of Solution Results</h3>\n",
    "\n",
    "<p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "    The project explored the effectiveness of both individual and hybrid models in fake news detection. Each individual model—DistilBERT, ELECTRA, and OPT—demonstrated strong performance metrics on the dataset. Among them, the <b>OPT Classifier</b> achieved the highest overall accuracy, F1 score, and ROC AUC, owing to its advanced architecture and fine-tuning capabilities. DistilBERT provided a strong balance between accuracy and efficiency, making it an excellent choice for lightweight applications, while ELECTRA excelled in its discriminator-based approach with competitive performance across metrics.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "    However, hybrid models combining discriminative and generative approaches did not consistently yield better results. While some combinations, like <b>OPT Classifier + GPT-2</b>, showed balanced accuracy for both real and fake news detection, others heavily favored one category. This suggests that the alignment between the generative and discriminative components requires further optimization. Additionally, overly aggressive preprocessing techniques may have stripped away valuable contextual information, limiting model performance.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 1.1em; text-align: justify;\">\n",
    "    Moving forward, the focus will be on improving the synergy between discriminative and generative components in hybrid models. This includes refining preprocessing pipelines to retain contextual richness. Enhancing data augmentation techniques to improve model robustness and generalization will also be a priority. Overall, this project highlighted the potential and challenges of hybrid approaches and provided valuable insights for future advancements in fake news detection.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e79bd9-e8dc-4769-b9ab-d709b16918a6",
   "metadata": {
    "id": "47e79bd9-e8dc-4769-b9ab-d709b16918a6"
   },
   "source": [
    "\n",
    "<h1 style=\"color: darkgoldenrod\">Software</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beed320-23a0-4664-9483-b0e626619d81",
   "metadata": {
    "id": "9beed320-23a0-4664-9483-b0e626619d81"
   },
   "source": [
    "<p>Please be sure to submit a requirements.txt file so that the instructors can easily run the code in this Jupyter Notebook.  The requirements.txt file should be in the same working directory as the Jupyter Notebook.  Also, please give any special instructions beyond the normal running of code in a Jupyter Notebook, such as where data files should be placed if not in the working directory of this Jupyter Notebook or if some of the installed software packages have additional requirements beyond \"pip install\".</p>\n",
    "\n",
    "<p>Each code cell should have contextually related code, such as a class, function implementing a major algorithm, or a set of short functions that support a larger function in a subsequent cell.  Code cells should also be present to show the performance/evaluation of a solution through well labeled graphs, tables, and/or performance measure values.</p>\n",
    "\n",
    "<p>The code cells also can be organized by each team member's solution.</p>\n",
    "\n",
    "<p>Each major set of related code cells should have the purpose of the code cells, the paper/informal references used (if any) to develop the code in the code cells, the team members who worked on the code cells, and major changes made to the code in the code cells by team members for this submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal\">Are there any special instructions for running the code in this Jupyter Notebook for this submission?</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>If Attempting to train models on Gpu Cuda:\n",
    "        pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118.</li>\n",
    "    <li>Make sure the Cuda Path in added to System environment</li>\n",
    "    <li>If Using Google Colab do Command 1 first</li>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5685df6-b71d-4026-9a73-4dac412877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND 1:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "# Define paths\n",
    "source_path = '/content/drive/MyDrive/Jupyter_Practice'\n",
    "target_path = '/content/workspace'\n",
    "\n",
    "# Check if the target path exists\n",
    "if os.path.exists(target_path) or os.path.islink(target_path):\n",
    "    # If it's a symbolic link or directory, remove it\n",
    "    try:\n",
    "        os.unlink(target_path)  # Remove symbolic link or file\n",
    "    except IsADirectoryError:\n",
    "        os.rmdir(target_path)  # Remove directory if it's not a link\n",
    "\n",
    "# Create the symbolic link\n",
    "os.symlink(source_path, target_path)\n",
    "\n",
    "# Change to the new workspace directory\n",
    "os.chdir(target_path)\n",
    "\n",
    "# Verify the symbolic link and list contents\n",
    "!ls /content/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e6faba-e744-4216-a5a7-55834d8773b0",
   "metadata": {
    "id": "88e6faba-e744-4216-a5a7-55834d8773b0"
   },
   "outputs": [],
   "source": [
    "#to capture all of the installed packages so far (run by the team to submit with the Jupyter notebook)\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b9ece-5297-4cb8-a663-7587f16459d5",
   "metadata": {
    "id": "6e9b9ece-5297-4cb8-a663-7587f16459d5"
   },
   "outputs": [],
   "source": [
    "#to install all of the packages in requirements.txt (run by the instructors when grading the notebook)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d298cad8-bed1-4d45-aeb2-e6ce5df3a0d9",
   "metadata": {
    "id": "d298cad8-bed1-4d45-aeb2-e6ce5df3a0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Steve\\anaconda3\\envs\\IR_2024\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Requirement already satisfied: datasets in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\steve\\anaconda3\\envs\\ir_2024\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "#packages to import to run the code in the Jupyter Notebook\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "!pip install datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "from transformers import EarlyStoppingCallback\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import GPT2Config\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd01d10-993d-49eb-831a-070269b02f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3097c61-78d2-4459-bbd2-13fa45051be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  # Tokenizer to remove punctuation by splitting on word characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41b413f-140b-4f98-86d0-dbf5451f6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "DISTILBERT_PATH = './model/distilbert'\n",
    "LLAMA_PATH = './model/llama'\n",
    "OPT_PATH = './model/opt'\n",
    "GPT2_PATH = './model/gpt2'\n",
    "ELECTRA_PATH = './model/electra'\n",
    "\n",
    "# Create model directories if they don't exist\n",
    "os.makedirs(DISTILBERT_PATH, exist_ok=True)\n",
    "os.makedirs(ELECTRA_PATH, exist_ok=True)\n",
    "os.makedirs(LLAMA_PATH, exist_ok=True)\n",
    "os.makedirs(OPT_PATH, exist_ok=True)\n",
    "os.makedirs(GPT2_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define directories for saving outputs\n",
    "OUTPUT_DIR = './outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "COMBINATIONS = ['distilbert_gpt2', 'distilbert_llama', 'distilbert_opt']\n",
    "for combination in COMBINATIONS:\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, combination), exist_ok=True)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 30\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b13f166-09b8-4f51-92e7-9e814f5a6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Check if text is not a string\n",
    "        text = \"\"  # Replace non-string values with an empty string\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"â\\w+|â€™|â€œ|â€|[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove content in brackets\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3a5bce-0f16-4a26-b131-857e784c91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all datasets\n",
    "file_paths = [\n",
    "    \"./data/Fake.csv\",\n",
    "    \"./data/True.csv\",\n",
    "    \"./data/BuzzFeed_fake_news_content.csv\",\n",
    "    \"./data/BuzzFeed_real_news_content.csv\",\n",
    "    \"./data/PolitiFact_fake_news_content.csv\",\n",
    "    \"./data/PolitiFact_real_news_content.csv\",\n",
    "    \"./data/cleaned_train.csv\",\n",
    "    \"./data/fake_or_real_news.csv\"\n",
    "]\n",
    "\n",
    "# Assign labels for files with only \"text\" column\n",
    "label_map = {\n",
    "    \"./data/Fake.csv\": 0,\n",
    "    \"./data/True.csv\": 1,\n",
    "    \"./data/BuzzFeed_fake_news_content.csv\": 0,\n",
    "    \"./data/BuzzFeed_real_news_content.csv\": 1,\n",
    "    \"./data/PolitiFact_fake_news_content.csv\": 0,\n",
    "    \"./data/PolitiFact_real_news_content.csv\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a92ef9a-80b8-492d-beb1-1270c7bb5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_file(file_path, label=None):\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "    try:\n",
    "        # Try reading the file with UTF-8 encoding\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"File {file_path} loaded successfully with utf-8 encoding.\")\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to ISO-8859-1 encoding \n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
    "        print(f\"File {file_path} loaded with ISO-8859-1 encoding.\")\n",
    "\n",
    "    # Debugging: Print initial column names\n",
    "    print(f\"Columns in {file_path}: {df.columns}\")\n",
    "\n",
    "    # Remove any unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Handle files with only \"text\" column\n",
    "    if 'text' in df.columns and 'label' not in df.columns:\n",
    "        print(f\"File {file_path} has only 'text' column. Assigning label: {label}\")\n",
    "        df['label'] = label  # Assign label based on the file (e.g., fake or real)\n",
    "\n",
    "    # Handle files with \"title\" and \"text\" columns\n",
    "    if 'title' in df.columns and 'text' in df.columns:\n",
    "        print(f\"File {file_path} has 'title' and 'text' columns. Combining them.\")\n",
    "        df['text'] = df['title'].fillna('') + \" \" + df['text'].fillna('')\n",
    "        df = df[['text', 'label']]  # Keep only necessary columns\n",
    "\n",
    "    # Debugging: Check unique label values before mapping\n",
    "    #print(f\"Unique label values before mapping: {df['label'].unique()}\")\n",
    "\n",
    "    # Map labels to binary values if they are in string format\n",
    "    if 'label' in df.columns and df['label'].dtype == 'object':\n",
    "        print(f\"Mapping string labels to binary values for {file_path}.\")\n",
    "        df['label'] = df['label'].map({'REAL': 1, 'FAKE': 0, 'real': 1, 'fake': 0})\n",
    "\n",
    "    # Debugging: Check unique label values after mapping\n",
    "    print(f\"Unique label values after mapping: {df['label'].unique()}\")\n",
    "\n",
    "    # Drop rows with missing text or label\n",
    "    print(f\"Initial shape of dataset: {df.shape}\")\n",
    "    df.dropna(subset=['text', 'label'], inplace=True)\n",
    "\n",
    "    # Ensure 'label' contains only 0 or 1\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')  # Convert to numeric\n",
    "    df = df[df['label'].isin([0, 1])]  # Keep only rows where label is 0 or 1\n",
    "    print(f\"Shape after filtering invalid labels: {df.shape}\")\n",
    "\n",
    "    # Ensure 'text' column contains strings\n",
    "    if 'text' in df.columns:\n",
    "        df['text'] = df['text'].astype(str)\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "    # Preprocess the text\n",
    "    print(f\"Preprocessing text for {file_path}.\")\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Debugging Outputs\n",
    "    print(f\"Processed head of the dataset for {file_path}:\\n\", df.head())\n",
    "    print(f\"Label value counts for {file_path}:\\n\", df['label'].value_counts(dropna=False))\n",
    "    print(f\"Final null value counts for {file_path}:\\n\", df.isnull().sum())\n",
    "\n",
    "    return df[['text', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548026f6-672a-439b-822b-18e2e93ea7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: ./data/Fake.csv\n",
      "File ./data/Fake.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/Fake.csv: Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "File ./data/Fake.csv has only 'text' column. Assigning label: 0\n",
      "File ./data/Fake.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [0]\n",
      "Initial shape of dataset: (23481, 2)\n",
      "Shape after filtering invalid labels: (23481, 2)\n",
      "Preprocessing text for ./data/Fake.csv.\n",
      "Processed head of the dataset for ./data/Fake.csv:\n",
      "                                                 text  label\n",
      "0  donald trump sends embarrassing new year eve m...      0\n",
      "1  drunk bragging trump staffer started russian c...      0\n",
      "2  sheriff david clarke becomes internet joke thr...      0\n",
      "3  trump obsessed even obamas name coded website ...      0\n",
      "4  pope francis called donald trump christmas spe...      0\n",
      "Label value counts for ./data/Fake.csv:\n",
      " label\n",
      "0    23481\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/Fake.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/True.csv\n",
      "File ./data/True.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/True.csv: Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
      "File ./data/True.csv has only 'text' column. Assigning label: 1\n",
      "File ./data/True.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [1]\n",
      "Initial shape of dataset: (21417, 2)\n",
      "Shape after filtering invalid labels: (21417, 2)\n",
      "Preprocessing text for ./data/True.csv.\n",
      "Processed head of the dataset for ./data/True.csv:\n",
      "                                                 text  label\n",
      "0  u budget fight loom republican flip fiscal scr...      1\n",
      "1  u military accept transgender recruit monday p...      1\n",
      "2  senior u republican senator let mr mueller job...      1\n",
      "3  fbi russia probe helped australian diplomat ti...      1\n",
      "4  trump want postal service charge much amazon s...      1\n",
      "Label value counts for ./data/True.csv:\n",
      " label\n",
      "1    21417\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/True.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/BuzzFeed_fake_news_content.csv\n",
      "File ./data/BuzzFeed_fake_news_content.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/BuzzFeed_fake_news_content.csv: Index(['id', 'title', 'text', 'url', 'top_img', 'authors', 'source',\n",
      "       'publish_date', 'movies', 'images', 'canonical_link', 'meta_data'],\n",
      "      dtype='object')\n",
      "File ./data/BuzzFeed_fake_news_content.csv has only 'text' column. Assigning label: 0\n",
      "File ./data/BuzzFeed_fake_news_content.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [0]\n",
      "Initial shape of dataset: (91, 2)\n",
      "Shape after filtering invalid labels: (91, 2)\n",
      "Preprocessing text for ./data/BuzzFeed_fake_news_content.csv.\n",
      "Processed head of the dataset for ./data/BuzzFeed_fake_news_content.csv:\n",
      "                                                 text  label\n",
      "0  proof mainstream medium manipulating election ...      0\n",
      "1  charity clinton foundation distributed watered...      0\n",
      "2  hillary clinton administration may entirely ru...      0\n",
      "3  trump latest campaign promise may horrible one...      0\n",
      "4            website maintenance website maintenance      0\n",
      "Label value counts for ./data/BuzzFeed_fake_news_content.csv:\n",
      " label\n",
      "0    91\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/BuzzFeed_fake_news_content.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/BuzzFeed_real_news_content.csv\n",
      "File ./data/BuzzFeed_real_news_content.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/BuzzFeed_real_news_content.csv: Index(['id', 'title', 'text', 'url', 'top_img', 'authors', 'source',\n",
      "       'publish_date', 'movies', 'images', 'canonical_link', 'meta_data'],\n",
      "      dtype='object')\n",
      "File ./data/BuzzFeed_real_news_content.csv has only 'text' column. Assigning label: 1\n",
      "File ./data/BuzzFeed_real_news_content.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [1]\n",
      "Initial shape of dataset: (91, 2)\n",
      "Shape after filtering invalid labels: (91, 2)\n",
      "Preprocessing text for ./data/BuzzFeed_real_news_content.csv.\n",
      "Processed head of the dataset for ./data/BuzzFeed_real_news_content.csv:\n",
      "                                                 text  label\n",
      "0  another terrorist attack nycwhy still politica...      1\n",
      "1  donald trump drug big factor charlotte protest...      1\n",
      "2  obama un giving liberty enhances security amer...      1\n",
      "3  trump v clinton fundamental clash economy work...      1\n",
      "4  president obama veto 911 victim bill setting s...      1\n",
      "Label value counts for ./data/BuzzFeed_real_news_content.csv:\n",
      " label\n",
      "1    91\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/BuzzFeed_real_news_content.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/PolitiFact_fake_news_content.csv\n",
      "File ./data/PolitiFact_fake_news_content.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/PolitiFact_fake_news_content.csv: Index(['id', 'title', 'text', 'url', 'top_img', 'authors', 'source',\n",
      "       'publish_date', 'movies', 'images', 'canonical_link', 'meta_data'],\n",
      "      dtype='object')\n",
      "File ./data/PolitiFact_fake_news_content.csv has only 'text' column. Assigning label: 0\n",
      "File ./data/PolitiFact_fake_news_content.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [0]\n",
      "Initial shape of dataset: (120, 2)\n",
      "Shape after filtering invalid labels: (120, 2)\n",
      "Preprocessing text for ./data/PolitiFact_fake_news_content.csv.\n",
      "Processed head of the dataset for ./data/PolitiFact_fake_news_content.csv:\n",
      "                                                 text  label\n",
      "0  trump insulted million lost everything bush re...      0\n",
      "1  famous dog killed spot waited year owner retur...      0\n",
      "2  house oversight panel vote clinton chief conte...      0\n",
      "3  america tragically lost country music icon ple...      0\n",
      "4  monument battle new south nine year ago driver...      0\n",
      "Label value counts for ./data/PolitiFact_fake_news_content.csv:\n",
      " label\n",
      "0    120\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/PolitiFact_fake_news_content.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/PolitiFact_real_news_content.csv\n",
      "File ./data/PolitiFact_real_news_content.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/PolitiFact_real_news_content.csv: Index(['id', 'title', 'text', 'url', 'top_img', 'authors', 'source',\n",
      "       'publish_date', 'movies', 'images', 'canonical_link', 'meta_data'],\n",
      "      dtype='object')\n",
      "File ./data/PolitiFact_real_news_content.csv has only 'text' column. Assigning label: 1\n",
      "File ./data/PolitiFact_real_news_content.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [1]\n",
      "Initial shape of dataset: (120, 2)\n",
      "Shape after filtering invalid labels: (120, 2)\n",
      "Preprocessing text for ./data/PolitiFact_real_news_content.csv.\n",
      "Processed head of the dataset for ./data/PolitiFact_real_news_content.csv:\n",
      "                                                 text  label\n",
      "0  trump insulted million lost everything bush re...      1\n",
      "1  famous dog killed spot waited year owner retur...      1\n",
      "2  house oversight panel vote clinton chief conte...      1\n",
      "3  america tragically lost country music icon ple...      1\n",
      "4  monument battle new south nine year ago driver...      1\n",
      "Label value counts for ./data/PolitiFact_real_news_content.csv:\n",
      " label\n",
      "1    120\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/PolitiFact_real_news_content.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/cleaned_train.csv\n",
      "File ./data/cleaned_train.csv loaded with ISO-8859-1 encoding.\n",
      "Columns in ./data/cleaned_train.csv: Index(['id', 'title', 'author', 'text', 'label', 'Helper', 'KEEP'], dtype='object')\n",
      "File ./data/cleaned_train.csv has 'title' and 'text' columns. Combining them.\n",
      "Unique label values after mapping: [ 1.  0. nan]\n",
      "Initial shape of dataset: (24212, 2)\n",
      "Shape after filtering invalid labels: (20707, 2)\n",
      "Preprocessing text for ./data/cleaned_train.csv.\n",
      "Processed head of the dataset for ./data/cleaned_train.csv:\n",
      "                                                 text  label\n",
      "0  house dem aide didnt even see comeys letter ja...      1\n",
      "1  flynn hillary clinton big woman campus breitba...      0\n",
      "2  truth might get fired truth might get fired oc...      1\n",
      "3  15 civilian killed single u airstrike identifi...      1\n",
      "4  iranian woman jailed fictional unpublished sto...      1\n",
      "Label value counts for ./data/cleaned_train.csv:\n",
      " label\n",
      "1    10362\n",
      "0    10345\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/cleaned_train.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Processing file: ./data/fake_or_real_news.csv\n",
      "File ./data/fake_or_real_news.csv loaded successfully with utf-8 encoding.\n",
      "Columns in ./data/fake_or_real_news.csv: Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n",
      "File ./data/fake_or_real_news.csv has 'title' and 'text' columns. Combining them.\n",
      "Mapping string labels to binary values for ./data/fake_or_real_news.csv.\n",
      "Unique label values after mapping: [0 1]\n",
      "Initial shape of dataset: (6335, 2)\n",
      "Shape after filtering invalid labels: (6335, 2)\n",
      "Preprocessing text for ./data/fake_or_real_news.csv.\n",
      "Processed head of the dataset for ./data/fake_or_real_news.csv:\n",
      "                                                 text  label\n",
      "0  smell hillary fear daniel greenfield shillman ...      0\n",
      "1  watch exact moment paul ryan committed politic...      0\n",
      "2  kerry go paris gesture sympathy u secretary st...      1\n",
      "3  bernie supporter twitter erupt anger dnc tried...      0\n",
      "4  battle new york primary matter primary day new...      1\n",
      "Label value counts for ./data/fake_or_real_news.csv:\n",
      " label\n",
      "1    3171\n",
      "0    3164\n",
      "Name: count, dtype: int64\n",
      "Final null value counts for ./data/fake_or_real_news.csv:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "final_data = pd.DataFrame()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    label = label_map.get(file_path, None)  \n",
    "    df = load_and_preprocess_file(file_path, label=label)\n",
    "    final_data = pd.concat([final_data, df], ignore_index=True)\n",
    "\n",
    "# Shuffle and reset index\n",
    "final_data = final_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca09995-1c6d-412e-b6c8-1a9d79d2d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# 0. GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec84e6c3-5a0e-4262-9bf2-d039e4fdd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenization function\n",
    "def tokenize_data(texts, tokenizer, max_length=128):\n",
    "    if isinstance(texts, list):\n",
    "        texts = [str(text) if text is not None else \"\" for text in texts]\n",
    "    else:\n",
    "        texts = str(texts) if texts is not None else \"\"\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa5f741-4afd-4131-ac70-7daa6a013745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and preprocess datasets for training\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    final_data[\"text\"].tolist(),\n",
    "    final_data[\"label\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=final_data[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12414e52-4af7-49f3-9a7e-5e458296c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_dir(base_path):\n",
    "    model_dirs = glob.glob(os.path.join(base_path, \"*\"))\n",
    "    if not model_dirs:\n",
    "        return None\n",
    "    latest_dir = max(model_dirs, key=os.path.getmtime)\n",
    "    return latest_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b417c-7524-48a5-b65d-ed61d725011a",
   "metadata": {},
   "source": [
    "### Training Models Code and Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "539973e8-b757-48a2-a62c-acd353810d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DistilBERT-based model\n",
    "def train_distilbert(train_texts, train_labels, test_texts, test_labels, epochs, batch_size, output_dir='./model/distilbert'):\n",
    "    print(\"Training DistilBERT\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "    # Tokenize training and testing data\n",
    "    train_encodings = tokenize_data(train_texts, tokenizer, max_length=128)\n",
    "    test_encodings = tokenize_data(test_texts, tokenizer, max_length=128)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(train_labels, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(test_labels, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "    # Calculate dynamic steps\n",
    "    total_train_samples = len(train_texts)\n",
    "    print(f\"total_train_samples: {total_train_samples}\")\n",
    "    steps_per_epoch = math.ceil(total_train_samples / batch_size)\n",
    "    print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "    eval_steps = 7 * steps_per_epoch // 100  # Evaluate every ~7% of an epoch \n",
    "\n",
    "    print(f\"Total train samples: {total_train_samples}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Eval steps: {eval_steps}\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps ,  \n",
    "        save_steps= 8 * eval_steps,  \n",
    "        gradient_accumulation_steps=2,  \n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,  # L2 regularization\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=int(0.1 * steps_per_epoch),  # 10% warmup steps\n",
    "        logging_dir='./distilbert_logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False,  # Ensure GPU usage\n",
    "        dataloader_num_workers=2  \n",
    "    )\n",
    "\n",
    "    # Compute metrics function\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Initialize Trainer with EarlyStoppingCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],  # Early stopping\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"DistilBERT model saved to {output_dir}\")\n",
    "\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "\n",
    "# Load DistilBERT model\n",
    "def load_distilbert():\n",
    "    print(f\"Loading DistilBERT model from {DISTILBERT_PATH}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(DISTILBERT_PATH).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DISTILBERT_PATH)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac7a0c44-7056-4d49-a9fb-fcb06faf0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt2(train_texts, test_texts, epochs, batch_size):\n",
    "    print(\"Training distilgpt2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a padding token, using EOS token\n",
    "    config = GPT2Config.from_pretrained(\"distilgpt2\")\n",
    "    config.attn_pdrop = 0.2  \n",
    "    config.resid_pdrop = 0.2  \n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", config=config).to(device)\n",
    "\n",
    "    # Tokenize training and testing data\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Set the labels to be the same as input IDs for causal language modeling\n",
    "    train_encodings['labels'] = train_encodings['input_ids'].clone()\n",
    "    test_encodings['labels'] = test_encodings['input_ids'].clone()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': train_encodings['labels'],\n",
    "    })\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask'],\n",
    "        'labels': test_encodings['labels'],\n",
    "    })\n",
    "\n",
    "    # Calculate dynamic steps\n",
    "    total_train_samples = len(train_texts)\n",
    "    print(f\"total_train_samples: {total_train_samples}\")\n",
    "    steps_per_epoch = math.ceil(total_train_samples / batch_size)\n",
    "    print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "    eval_steps = math.ceil(0.20 * steps_per_epoch)  # Evaluate every ~7% of an epoch\n",
    "\n",
    "    # Set output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(GPT2_PATH, f\"gpt2_{timestamp}\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        # per_device_train_batch_size=batch_size,\n",
    "        # per_device_eval_batch_size=batch_size,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",  \n",
    "        eval_steps=eval_steps,  \n",
    "        save_steps= 3 * eval_steps,  \n",
    "        gradient_accumulation_steps=2,  \n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        warmup_steps=int(0.1 * steps_per_epoch),  # Warm-up for 10% of steps\n",
    "        logging_dir='./gpt2_logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",  \n",
    "        greater_is_better=False,  # Loss should decrease\n",
    "        fp16=torch.cuda.is_available(),  \n",
    "        dataloader_num_workers=2,  \n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with EarlyStoppingCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],  # Stop after 4 evaluations without improvement\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"GPT-2 model saved to {output_dir}\")\n",
    "\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Load the latest GPT-2 model\n",
    "def load_gpt2():\n",
    "    try:\n",
    "        model_dirs = [\n",
    "            os.path.join(GPT2_PATH, d) for d in os.listdir(GPT2_PATH) if d.startswith(\"gpt2_\")\n",
    "        ]\n",
    "        if not model_dirs:\n",
    "            raise FileNotFoundError(f\"No directories starting with 'gpt2_' found in {GPT2_PATH}\")\n",
    "        \n",
    "        latest_path = max(model_dirs, key=os.path.getmtime)\n",
    "\n",
    "        print(f\"Attempting to load the latest GPT-2 model from: {latest_path}\")\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(latest_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(latest_path)\n",
    "\n",
    "        print(f\"Successfully loaded GPT-2 model and tokenizer from {latest_path}\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GPT-2 model: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f13a760-5549-4d77-a4af-d4b5bfd28e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELECTRA Training Function\n",
    "def train_electra(train_texts, train_labels, test_texts, test_labels, epochs, batch_size):\n",
    "    print(\"Training ELECTRA\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=2).to(device)\n",
    "\n",
    "    # Tokenize training and testing data\n",
    "    train_encodings = tokenize_data(train_texts, tokenizer, max_length=128)\n",
    "    test_encodings = tokenize_data(test_texts, tokenizer, max_length=128)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(train_labels, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(test_labels, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "    # Calculate dynamic steps\n",
    "    total_train_samples = len(train_texts)\n",
    "    print(f\"total_train_samples: {total_train_samples}\")\n",
    "    steps_per_epoch = math.ceil(total_train_samples / batch_size)\n",
    "    print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "    eval_steps = math.ceil(0.10 * steps_per_epoch)\n",
    "\n",
    "    # Set output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(ELECTRA_PATH, f\"electra_{timestamp}\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps ,  \n",
    "        save_steps=5 * eval_steps,  \n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,  # L2 regularization\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=int(0.1 * steps_per_epoch),  # 10% of steps for warmup\n",
    "        logging_dir='./electra_logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False,# Ensure GPU-only\n",
    "        dataloader_num_workers=2\n",
    "    )\n",
    "\n",
    "    # Define compute metrics function\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Initialize the Trainer with EarlyStoppingCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"ELECTRA model saved to {output_dir}\")\n",
    "\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "def load_electra():\n",
    "    print(\"Loading the latest ELECTRA model...\")\n",
    "    checkpoint_dirs = [\n",
    "        os.path.join(ELECTRA_PATH, d) for d in os.listdir(ELECTRA_PATH) if d.startswith(\"electra_\")\n",
    "    ]\n",
    "    if not checkpoint_dirs:\n",
    "        raise FileNotFoundError(f\"No ELECTRA checkpoints found in {ELECTRA_PATH}\")\n",
    "\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "    print(f\"Latest checkpoint directory found: {latest_checkpoint}\")\n",
    "\n",
    "    # Load the model and tokenizer from the latest checkpoint\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)\n",
    "        print(f\"ELECTRA model and tokenizer loaded successfully from {latest_checkpoint}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to load ELECTRA model from {latest_checkpoint}. Error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1093e43-a4e8-4574-ad91-aba98dc45de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_opt_classifier(train_texts, train_labels, test_texts, test_labels, epochs, batch_size):\n",
    "    print(\"Training OPT as a Classifier\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-125m\", num_labels=2).to(device)\n",
    "\n",
    "    # Tokenize training and testing data\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(train_labels)\n",
    "    })\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask'],\n",
    "        'labels': torch.tensor(test_labels)\n",
    "    })\n",
    "\n",
    "    # Calculate dynamic steps\n",
    "    total_train_samples = len(train_texts)\n",
    "    print(f\"total_train_samples: {total_train_samples}\")\n",
    "    steps_per_epoch = math.ceil(total_train_samples / batch_size)\n",
    "    print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "    eval_steps = math.ceil(0.10 * steps_per_epoch)  # Evaluate every ~7% of an epoch\n",
    "\n",
    "    # Set output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(OPT_PATH, f\"opt_classifier_{timestamp}\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=3 * eval_steps,  # Save the model every evaluation 56%\n",
    "        gradient_accumulation_steps=2,  \n",
    "        learning_rate=3e-5,  \n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=int(0.1 * steps_per_epoch),  # Warm-up 10% of steps\n",
    "        logging_dir='./opt_classifier_logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False, # Ensure GPU usage\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2\n",
    "    )\n",
    "\n",
    "    # Define evaluation metrics\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],  # Stop after 5 evaluations without improvement\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"OPT Classifier model saved to {output_dir}\")\n",
    "\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def load_opt_classifier():\n",
    "    try:\n",
    "        model_dirs = [\n",
    "            os.path.join(OPT_PATH, d) for d in os.listdir(OPT_PATH) if d.startswith(\"opt_classifier_\")\n",
    "        ]\n",
    "        if not model_dirs:\n",
    "            raise FileNotFoundError(f\"No directories starting with 'opt_classifier_' found in {OPT_PATH}\")\n",
    "\n",
    "        latest_path = max(model_dirs, key=os.path.getmtime)\n",
    "\n",
    "        print(f\"Attempting to load the latest OPT Classifier model from: {latest_path}\")\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(latest_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(latest_path)\n",
    "\n",
    "        print(f\"Successfully loaded OPT Classifier model and tokenizer from {latest_path}\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading OPT Classifier model: {str(e)}\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d408277-bd5c-46ef-8996-4e0d9bff8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_opt_generator(train_texts, test_texts, epochs, batch_size):\n",
    "    print(\"Training OPT as a Generator\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "\n",
    "    # Tokenize training and testing data\n",
    "    train_encodings = tokenizer(train_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer(test_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': train_encodings['input_ids']\n",
    "    })\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask'],\n",
    "        'labels': test_encodings['input_ids']\n",
    "    })\n",
    "\n",
    "    # Calculate dynamic steps\n",
    "    total_train_samples = len(train_texts)\n",
    "    print(f\"total_train_samples: {total_train_samples}\")\n",
    "    steps_per_epoch = math.ceil(total_train_samples / batch_size)\n",
    "    print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "    eval_steps = math.ceil(0.07 * steps_per_epoch)  # Evaluate every ~7% of an epoch\n",
    "\n",
    "\n",
    "    # Set output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(OPT_PATH, f\"opt_generator_{timestamp}\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",  \n",
    "        eval_steps=eval_steps, \n",
    "        save_steps= 8 * eval_steps,  \n",
    "        gradient_accumulation_steps=2,  \n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=int(0.1* steps_per_epoch),  \n",
    "        logging_dir='./opt_generator_logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        no_cuda=False, # Ensure GPU usage\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  \n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"OPT Generator model saved to {output_dir}\")\n",
    "\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def load_opt_generator():\n",
    "    try:\n",
    "        # List all directories in the OPT_PATH that match the naming convention\n",
    "        model_dirs = [\n",
    "            os.path.join(OPT_PATH, d) for d in os.listdir(OPT_PATH) if d.startswith(\"opt_generator_\")\n",
    "        ]\n",
    "        if not model_dirs:\n",
    "            raise FileNotFoundError(f\"No directories starting with 'opt_generator_' found in {OPT_PATH}\")\n",
    "\n",
    "        # Find the latest model directory by modification time\n",
    "        latest_path = max(model_dirs, key=os.path.getmtime)\n",
    "\n",
    "        print(f\"Attempting to load the latest OPT Generator model from: {latest_path}\")\n",
    "\n",
    "        # Load the model and tokenizer from the latest directory\n",
    "        model = AutoModelForCausalLM.from_pretrained(latest_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(latest_path)\n",
    "\n",
    "        print(f\"Successfully loaded OPT Generator model and tokenizer from {latest_path}\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading OPT Generator model: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8610df8-de08-4fb9-8a5a-06e94fd7f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training Phase\n",
    "bert_trainer, bert_model, bert_tokenizer = train_distilbert(train_texts, train_labels, test_texts, test_labels, num_epochs, batch_size)\n",
    "\n",
    "# Train GPT-2 model\n",
    "torch.cuda.empty_cache()\n",
    "gpt2_trainer, gpt2_model, gpt2_tokenizer = train_gpt2(train_texts, test_texts, num_epochs, batch_size)\n",
    "\n",
    "# Train ELECTRA model\n",
    "electra_trainer, electra_model, electra_tokenizer = train_electra(train_texts, train_labels, test_texts, test_labels, num_epochs, batch_size)\n",
    "\n",
    "# Train OPT Classifier model\n",
    "opt_classifier_trainer, opt_classifier_model, opt_classifier_tokenizer = train_opt_classifier(train_texts, train_labels, test_texts, test_labels, num_epochs, batch_size)\n",
    "\n",
    "# Train OPT Generator model\n",
    "opt_generator_trainer, opt_generator_model, opt_generator_tokenizer = train_opt_generator(train_texts, test_texts, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3a307-f36b-4f64-9b44-ce0e35709b94",
   "metadata": {},
   "source": [
    "## Hybrid Detection Models for Robust Detection of Disinformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565039db-a366-44bf-adf9-f5928380d74a",
   "metadata": {
    "id": "565039db-a366-44bf-adf9-f5928380d74a"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block : DistilBERT + GPT-2</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    The purpose of this code block is to implement a hybrid detection model that leverages the strengths of both \n",
    "    discriminative (DistilBERT) and generative (GPT-2) transformer models for fake news detection. DistilBERT is used for \n",
    "    binary classification, predicting whether the input text is real or fake, while GPT-2 generates semantically similar \n",
    "    text to enhance contextual understanding. The model combines the outputs of both components using cosine similarity \n",
    "    to determine the final classification, aiming to improve overall detection performance.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/10/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db7b45c6-a6d2-4dcf-9941-6e15bb2697b9",
   "metadata": {
    "id": "db7b45c6-a6d2-4dcf-9941-6e15bb2697b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Detection Model with DistilBERT and GPT-2\n",
    "def detect_with_distilbert_gpt2(text, bert_model, bert_tokenizer, gpt_model, gpt_tokenizer, similarity_threshold=0.77):\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # DistilBERT prediction\n",
    "    bert_inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    bert_outputs = bert_model(input_ids=bert_inputs['input_ids'], attention_mask=bert_inputs['attention_mask'], output_hidden_states=True)\n",
    "    bert_prediction = torch.argmax(bert_outputs.logits, dim=1).item()\n",
    "\n",
    "    # GPT-2 text generation \n",
    "    gpt_inputs = gpt_tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True  \n",
    "    )\n",
    "    gpt_inputs = gpt_inputs.to(device)\n",
    "    gpt_outputs = gpt_model.generate(\n",
    "        gpt_inputs['input_ids'],\n",
    "        attention_mask=gpt_inputs['attention_mask'], \n",
    "        max_new_tokens=50, \n",
    "        pad_token_id=gpt_tokenizer.pad_token_id\n",
    "    )\n",
    "    generated_text = gpt_tokenizer.decode(gpt_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # BERT prediction on GPT-2-generated text\n",
    "    generated_bert_inputs = bert_tokenizer(generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    generated_bert_outputs = bert_model(input_ids=generated_bert_inputs['input_ids'], attention_mask=generated_bert_inputs['attention_mask'], output_hidden_states=True)\n",
    "\n",
    "    # Cosine similarity between original and generated text embeddings\n",
    "    bert_embedding = bert_outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "    generated_bert_embedding = generated_bert_outputs.hidden_states[-1][:, 0, :]\n",
    "    similarity = torch.nn.functional.cosine_similarity(bert_embedding, generated_bert_embedding, dim=1).item()\n",
    "\n",
    "    if bert_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff23094-b28f-4c32-b5e4-b6d38b3fb886",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - DistilBERT + OPT-Generator</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell implements a hybrid detection model that combines DistilBERT's discriminative capabilities \n",
    "    with the generative potential of OPT-Generator. The model aims to enhance fake news detection by leveraging \n",
    "    contextual improvements from generated text while maintaining robust classification performance. \n",
    "    Cosine similarity is employed to evaluate the alignment between the original and generated embeddings.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/10/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8494c36-9681-4125-9b60-0f2e3b247951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_distilbert_opt_generator(text, distilbert_model, distilbert_tokenizer, opt_generator_model, opt_generator_tokenizer, similarity_threshold=0.77):\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # DistilBERT prediction\n",
    "    bert_inputs = distilbert_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "    bert_outputs = distilbert_model(\n",
    "        input_ids=bert_inputs['input_ids'],\n",
    "        attention_mask=bert_inputs['attention_mask'],\n",
    "        output_hidden_states=True  \n",
    "    )\n",
    "    bert_prediction = torch.argmax(bert_outputs.logits, dim=1).item()\n",
    "\n",
    "    # OPT Generator text generation using max_new_tokens\n",
    "    generator_inputs = opt_generator_tokenizer.encode_plus(\n",
    "        text, return_tensors='pt', max_length=128, truncation=True, padding=True\n",
    "    ).to(device)\n",
    "    generator_outputs = opt_generator_model.generate(\n",
    "        generator_inputs['input_ids'],\n",
    "        attention_mask=generator_inputs['attention_mask'],\n",
    "        max_new_tokens=50,  \n",
    "        pad_token_id=opt_generator_tokenizer.pad_token_id\n",
    "    )\n",
    "    generated_text = opt_generator_tokenizer.decode(generator_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # DistilBERT prediction on OPT-generated text\n",
    "    generated_bert_inputs = distilbert_tokenizer(\n",
    "        generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "    generated_bert_outputs = distilbert_model(\n",
    "        input_ids=generated_bert_inputs['input_ids'],\n",
    "        attention_mask=generated_bert_inputs['attention_mask'],\n",
    "        output_hidden_states=True  \n",
    "    )\n",
    "\n",
    "    # Cosine similarity between original and generated text embeddings\n",
    "    original_embedding = bert_outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "    generated_embedding = generated_bert_outputs.hidden_states[-1][:, 0, :]  \n",
    "    similarity = torch.nn.functional.cosine_similarity(original_embedding, generated_embedding, dim=-1).item()\n",
    "\n",
    "    # Return detection result\n",
    "    if bert_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n",
    "\n",
    "\n",
    "def detect_with_distilbert_opt(text, bert_model, bert_tokenizer, opt_model, opt_tokenizer, similarity_threshold=0.77):\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # DistilBERT prediction\n",
    "    bert_inputs = bert_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=64\n",
    "    ).to(device)\n",
    "    bert_outputs = bert_model(\n",
    "        input_ids=bert_inputs['input_ids'],\n",
    "        attention_mask=bert_inputs['attention_mask'],\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    bert_prediction = torch.argmax(bert_outputs.logits, dim=1).item()\n",
    "\n",
    "    # OPT text generation using max_new_tokens\n",
    "    opt_inputs = opt_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=64\n",
    "    ).to(device)\n",
    "    opt_outputs = opt_model.generate(\n",
    "        opt_inputs['input_ids'],\n",
    "        attention_mask=opt_inputs['attention_mask'],\n",
    "        max_new_tokens=50,  \n",
    "        pad_token_id=opt_tokenizer.pad_token_id\n",
    "    )\n",
    "    generated_text = opt_tokenizer.decode(opt_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # DistilBERT prediction on OPT-generated text\n",
    "    generated_bert_inputs = bert_tokenizer(\n",
    "        generated_text, return_tensors='pt', truncation=True, padding=True, max_length=64\n",
    "    ).to(device)\n",
    "    generated_bert_outputs = bert_model(\n",
    "        input_ids=generated_bert_inputs['input_ids'],\n",
    "        attention_mask=generated_bert_inputs['attention_mask'],\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    # Cosine similarity between original and generated text embeddings\n",
    "    bert_embedding = bert_outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "    generated_bert_embedding = generated_bert_outputs.hidden_states[-1][:, 0, :]\n",
    "    similarity = torch.nn.functional.cosine_similarity(bert_embedding, generated_bert_embedding, dim=1).item()\n",
    "\n",
    "    if bert_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654fc79-5ab8-4173-a958-173a8484855e",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - OPT Classifier + OPT Generator</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell implements a hybrid detection model combining OPT Classifier's discriminative capabilities \n",
    "    with OPT Generator's text generation abilities. \n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/12/2024 - major change made for this submission</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49c33590-5df9-4a72-9ea4-5385bbc1871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_opt_classifier_opt_generator(text, opt_classifier_model, opt_classifier_tokenizer, opt_generator_model, opt_generator_tokenizer, similarity_threshold=0.77):\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # OPT Classifier prediction\n",
    "    classifier_inputs = opt_classifier_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "    classifier_outputs = opt_classifier_model(\n",
    "        input_ids=classifier_inputs['input_ids'],\n",
    "        attention_mask=classifier_inputs['attention_mask'],\n",
    "        output_hidden_states=True  # Ensure hidden states are returned\n",
    "    )\n",
    "    classifier_prediction = torch.argmax(classifier_outputs.logits, dim=1).item()\n",
    "\n",
    "    # OPT Generator text generation\n",
    "    generator_inputs = opt_generator_tokenizer.encode_plus(\n",
    "        text, return_tensors='pt', max_length=128, truncation=True, padding=True\n",
    "    ).to(device)\n",
    "    generator_outputs = opt_generator_model.generate(\n",
    "        generator_inputs['input_ids'],\n",
    "        attention_mask=generator_inputs['attention_mask'],\n",
    "        max_new_tokens=50,  # Generate 50 new tokens beyond the input\n",
    "        pad_token_id=opt_generator_tokenizer.pad_token_id\n",
    "    )\n",
    "    generated_text = opt_generator_tokenizer.decode(generator_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # OPT Classifier prediction on OPT-generated text\n",
    "    generated_classifier_inputs = opt_classifier_tokenizer(\n",
    "        generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "    generated_classifier_outputs = opt_classifier_model(\n",
    "        input_ids=generated_classifier_inputs['input_ids'],\n",
    "        attention_mask=generated_classifier_inputs['attention_mask'],\n",
    "        output_hidden_states=True  # Ensure hidden states are returned\n",
    "    )\n",
    "\n",
    "    # Ensure hidden states are available\n",
    "    if classifier_outputs.hidden_states is None or generated_classifier_outputs.hidden_states is None:\n",
    "        raise ValueError(\"Model outputs do not contain hidden states. Ensure output_hidden_states=True.\")\n",
    "\n",
    "    # Extract embeddings from the classifier model for cosine similarity\n",
    "    original_embedding = classifier_outputs.hidden_states[-1][:, 0, :]  # [CLS] token\n",
    "    generated_embedding = generated_classifier_outputs.hidden_states[-1][:, 0, :]\n",
    "    similarity = torch.nn.functional.cosine_similarity(original_embedding, generated_embedding, dim=-1).item()\n",
    "\n",
    "    # Decide based on classification and similarity\n",
    "    if classifier_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2666d-c840-4827-833a-ecc8b5323960",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - OPT Classifier + GPT-2</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell implements a hybrid detection model that combines the discriminative abilities of OPT Classifier \n",
    "    with the contextual reasoning capabilities of GPT-2. The primary goal is to enhance fake news detection by leveraging \n",
    "    the generative text outputs of GPT-2 and validating them through OPT Classifier predictions. \n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/12/2024 - major change made for this submission</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c97de8ac-38d7-4be1-976b-bc5a767f1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_opt_classifier_gpt2(text, opt_classifier_model, opt_classifier_tokenizer, gpt_model, gpt_tokenizer, similarity_threshold=0.77):\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # OPT Classifier prediction\n",
    "    classifier_inputs = opt_classifier_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    classifier_outputs = opt_classifier_model(input_ids=classifier_inputs['input_ids'],attention_mask=classifier_inputs['attention_mask'])\n",
    "    classifier_prediction = torch.argmax(classifier_outputs.logits, dim=1).item()\n",
    "\n",
    "    # GPT-2 text generation\n",
    "    gpt_inputs = gpt_tokenizer.encode_plus(text, return_tensors='pt', max_length=128, truncation=True, padding=True).to(device)\n",
    "    gpt_outputs = gpt_model.generate(gpt_inputs['input_ids'],attention_mask=gpt_inputs['attention_mask'],max_new_tokens=50, pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "    generated_text = gpt_tokenizer.decode(gpt_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # OPT Classifier prediction on GPT-2-generated text\n",
    "    generated_classifier_inputs = opt_classifier_tokenizer(generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    generated_classifier_outputs = opt_classifier_model(input_ids=generated_classifier_inputs['input_ids'],attention_mask=generated_classifier_inputs['attention_mask'])\n",
    "\n",
    "    # Cosine similarity between original and generated text logits\n",
    "    original_logits = classifier_outputs.logits[:, 1]  # Real News logits\n",
    "    generated_logits = generated_classifier_outputs.logits[:, 1]\n",
    "    similarity = torch.nn.functional.cosine_similarity(original_logits, generated_logits, dim=0).item()\n",
    "\n",
    "    if classifier_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d6304-b368-4271-9f80-b8f63f2d2a7a",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - ELECTRA + GPT-2</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell implements a hybrid detection model that combines the discriminative strengths of ELECTRA with the contextual text generation capabilities of GPT-2. Cosine similarity between embeddings of the original and generated text is used to evaluate alignment and make final predictions.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/14/2024 - major change made for this submission</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "382c578a-0eba-452e-b733-d88dfe9ab2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_electra_gpt2(text, electra_model, electra_tokenizer, gpt2_model, gpt2_tokenizer, similarity_threshold=0.77):\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # ELECTRA prediction\n",
    "    electra_inputs = electra_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    electra_outputs = electra_model(\n",
    "        input_ids=electra_inputs['input_ids'],\n",
    "        attention_mask=electra_inputs['attention_mask'],\n",
    "        output_hidden_states=True  \n",
    "    )\n",
    "    electra_prediction = torch.argmax(electra_outputs.logits, dim=1).item()\n",
    "\n",
    "    # GPT-2 text generation \n",
    "    gpt_inputs = gpt2_tokenizer.encode_plus(\n",
    "        text, return_tensors='pt', max_length=128, truncation=True, padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    gpt_outputs = gpt2_model.generate(\n",
    "        gpt_inputs['input_ids'],\n",
    "        attention_mask=gpt_inputs['attention_mask'],\n",
    "        max_new_tokens=50,  \n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = gpt2_tokenizer.decode(gpt_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # ELECTRA prediction on GPT-2-generated text\n",
    "    generated_electra_inputs = electra_tokenizer(\n",
    "        generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    generated_electra_outputs = electra_model(\n",
    "        input_ids=generated_electra_inputs['input_ids'],\n",
    "        attention_mask=generated_electra_inputs['attention_mask'],\n",
    "        output_hidden_states=True  \n",
    "    )\n",
    "\n",
    "    # Cosine similarity between original and generated text embeddings\n",
    "    original_embedding = electra_outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "    generated_embedding = generated_electra_outputs.hidden_states[-1][:, 0, :]\n",
    "    similarity = torch.nn.functional.cosine_similarity(original_embedding, generated_embedding, dim=-1).item()\n",
    "\n",
    "    if electra_prediction == 0 or similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b660c-6ce8-4e5f-b58c-3ebb79cfc570",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - ELECTRA + OPT Generator</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell implements a hybrid detection model that leverages the strengths of ELECTRA's discriminative capabilities and OPT Generator's text generation abilities.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/14/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62bc29b2-78f0-4dc9-85f3-5915a75fd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_electra_opt_generator(\n",
    "    text, electra_model, electra_tokenizer, opt_generator_model, opt_generator_tokenizer, similarity_threshold=0.77\n",
    "):\n",
    "    # Text preprocessing\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # Electra prediction\n",
    "    electra_inputs = electra_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    electra_outputs = electra_model(\n",
    "        input_ids=electra_inputs['input_ids'],\n",
    "        attention_mask=electra_inputs['attention_mask'],\n",
    "        output_hidden_states=True,  # Ensure hidden states are returned\n",
    "    )\n",
    "\n",
    "    # OPT Generator text generation using max_new_tokens\n",
    "    generator_inputs = opt_generator_tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    generator_outputs = opt_generator_model.generate(\n",
    "        generator_inputs['input_ids'],\n",
    "        attention_mask=generator_inputs['attention_mask'],\n",
    "        max_new_tokens=50,  # Specify the number of new tokens to generate\n",
    "        pad_token_id=opt_generator_tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = opt_generator_tokenizer.decode(generator_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Electra prediction on OPT-generated text\n",
    "    generated_electra_inputs = electra_tokenizer(\n",
    "        generated_text, return_tensors='pt', truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    generated_electra_outputs = electra_model(\n",
    "        input_ids=generated_electra_inputs['input_ids'],\n",
    "        attention_mask=generated_electra_inputs['attention_mask'],\n",
    "        output_hidden_states=True,  # Ensure hidden states are returned\n",
    "    )\n",
    "\n",
    "    # Cosine similarity between original and generated text embeddings\n",
    "    original_embedding = electra_outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "    generated_embedding = generated_electra_outputs.hidden_states[-1][:, 0, :]\n",
    "    similarity = torch.nn.functional.cosine_similarity(original_embedding, generated_embedding, dim=-1).item()\n",
    "\n",
    "    # Return detection result\n",
    "    if similarity < similarity_threshold:\n",
    "        return \"Fake News Detected.\"\n",
    "    else:\n",
    "        return \"Real News Detected.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abb89-91ae-406d-a590-522c6fabb334",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Evaluation and Result Logging</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code cell provides a framework for evaluating hybrid or individual models for fake news detection. It systematically tests the detection function against labeled test data, logs the results for each input, and calculates the accuracy for both real and fake news detection. This evaluation mechanism ensures a transparent and reproducible assessment of the models' performance.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/14/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01a27e2c-108a-494f-b20b-e6f511031815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and save results \n",
    "def evaluate_and_save_results(test_texts, test_labels, detection_function, model_name, output_folder, *models_and_tokenizers):\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    real_count = 0\n",
    "    fake_count = 0\n",
    "    results = []\n",
    "\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    # Evaluate each test case\n",
    "    for i, (text, label) in enumerate(zip(test_texts, test_labels)):\n",
    "        result = detection_function(text, *models_and_tokenizers)\n",
    "        is_real_detected = result == \"Real News Detected.\"\n",
    "        is_fake_detected = result == \"Fake News Detected.\"\n",
    "\n",
    "        if label == 1 and is_real_detected:  \n",
    "            real_count += 1\n",
    "        elif label == 0 and is_fake_detected: \n",
    "            fake_count += 1\n",
    "\n",
    "        results.append((f\"News {i+1}\", text, result, \"Real\" if label == 1 else \"Fake\"))\n",
    "\n",
    "    # Save the results\n",
    "    #results_file = os.path.join(output_folder, f\"{model_name}_results.txt\")\n",
    "    results_file = os.path.join(output_folder, f\"{model_name}_results.txt_SubmissionExample\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        for news_id, text, result, true_label in results:\n",
    "            f.write(f\"{news_id}: {result} (True Label: {true_label})\\n\")\n",
    "            f.write(f\"Text: {text}\\n\\n\")\n",
    "\n",
    "    # Save the accuracy counts\n",
    "    accuracy_file = os.path.join(output_folder, f\"{model_name}_accuracy.txt\")\n",
    "    with open(accuracy_file, \"w\") as f:\n",
    "        f.write(f\"Real Accuracy count: {real_count}/{len([l for l in test_labels if l == 1])}\\n\")\n",
    "        f.write(f\"Fake Accuracy count: {fake_count}/{len([l for l in test_labels if l == 0])}\\n\")\n",
    "        print(f\"Real Accuracy count: {real_count}/{len([l for l in test_labels if l == 1])}\\n\")\n",
    "        print(f\"Fake Accuracy count: {fake_count}/{len([l for l in test_labels if l == 0])}\\n\")\n",
    "\n",
    "    print(f\"Results and accuracy counts for {model_name} saved to {output_folder}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09bfce85-1c64-492d-a507-81a9b0b6ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select fixed numbers of true and false texts\n",
    "def select_test_data(test_texts, test_labels, num_true=15, num_false=15):\n",
    "    true_texts = [text for text, label in zip(test_texts, test_labels) if label == 1]\n",
    "    false_texts = [text for text, label in zip(test_texts, test_labels) if label == 0]\n",
    "\n",
    "    # Randomly sample the required number of true and false cases\n",
    "    selected_true = random.sample(true_texts, num_true)\n",
    "    selected_false = random.sample(false_texts, num_false)\n",
    "\n",
    "    selected_texts = selected_true + selected_false\n",
    "    selected_labels = [1] * num_true + [0] * num_false\n",
    "\n",
    "    return selected_texts, selected_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b84af9-438a-450f-af7e-7eb9247bb075",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Evaluate Hybrid and Individual Models</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code block systematically evaluates hybrid and individual models for fake news detection on a balanced test dataset. \n",
    "    By combining discriminative models (e.g., DistilBERT, ELECTRA) with generative models (e.g., GPT-2, OPT-Generator), \n",
    "    the aim is to assess the synergy of these models. Each model's results, including real and fake news detection accuracy, \n",
    "    are saved to a dedicated output folder for further analysis and visualization.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/14/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c903008-b3ab-4b40-bece-2cc49c9a3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT model from ./model/distilbert\n",
      "Attempting to load the latest GPT-2 model from: ./model/gpt2\\gpt2_20241126_154614\n",
      "Successfully loaded GPT-2 model and tokenizer from ./model/gpt2\\gpt2_20241126_154614\n",
      "Attempting to load the latest OPT Classifier model from: ./model/opt\\opt_classifier_20241126_133043\n",
      "Successfully loaded OPT Classifier model and tokenizer from ./model/opt\\opt_classifier_20241126_133043\n",
      "Attempting to load the latest OPT Generator model from: ./model/opt\\opt_generator_20241126_155420\n",
      "Successfully loaded OPT Generator model and tokenizer from ./model/opt\\opt_generator_20241126_155420\n",
      "Loading the latest ELECTRA model...\n",
      "Latest checkpoint directory found: ./model/electra\\electra_20241126_172853\n",
      "ELECTRA model and tokenizer loaded successfully from ./model/electra\\electra_20241126_172853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models and tokenizers\n",
    "distilbert_model, distilbert_tokenizer = load_distilbert()\n",
    "distilbert_model.to(device)\n",
    "\n",
    "gpt2_model, gpt2_tokenizer = load_gpt2()\n",
    "gpt2_model.to(device)\n",
    "\n",
    "opt_classifier_model, opt_classifier_tokenizer = load_opt_classifier()\n",
    "opt_classifier_model.to(device)\n",
    "\n",
    "opt_generator_model, opt_generator_tokenizer = load_opt_generator()\n",
    "opt_generator_model.to(device)\n",
    "\n",
    "electra_model, electra_tokenizer = load_electra()\n",
    "electra_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a840301-0ba2-471b-8448-294ee02358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of texts to evaluate\n",
    "balanced_num = 5\n",
    "NUM_TRUE = balanced_num \n",
    "NUM_FALSE = balanced_num  \n",
    "\n",
    "# Select fixed test data\n",
    "selected_test_texts, selected_test_labels = select_test_data(test_texts, test_labels, num_true=NUM_TRUE, num_false=NUM_FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e04102e2-72d3-40c4-a9c5-d4545126b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert_opt_generator...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 3/5\n",
      "\n",
      "Results and accuracy counts for distilbert_opt_generator saved to ./outputs\\distilbert_opt\n",
      "\n",
      "Evaluating distilbert_gpt2...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 3/5\n",
      "\n",
      "Results and accuracy counts for distilbert_gpt2 saved to ./outputs\\distilbert_gpt2\n",
      "\n",
      "Evaluating distilbert_opt...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 3/5\n",
      "\n",
      "Results and accuracy counts for distilbert_opt saved to ./outputs\\distilbert_opt\n",
      "\n",
      "Evaluating electra_Generator...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 0/5\n",
      "\n",
      "Results and accuracy counts for electra_Generator saved to ./outputs\\electra_opt\n",
      "\n",
      "Evaluating electra_gpt2...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 1/5\n",
      "\n",
      "Results and accuracy counts for electra_gpt2 saved to ./outputs\\electra_gpt2\n",
      "\n",
      "Evaluating opt_classifier_opt_generator...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 4/5\n",
      "\n",
      "Results and accuracy counts for opt_classifier_opt_generator saved to ./outputs\\opt_classifier_opt_generator\n",
      "\n",
      "Evaluating opt_classifier_gpt2...\n",
      "Real Accuracy count: 5/5\n",
      "\n",
      "Fake Accuracy count: 4/5\n",
      "\n",
      "Results and accuracy counts for opt_classifier_gpt2 saved to ./outputs\\opt_classifier_gpt2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT + OPT-Generator\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_distilbert_opt_generator,\n",
    "    \"distilbert_opt_generator\",\n",
    "    os.path.join(OUTPUT_DIR, \"distilbert_opt\"),\n",
    "    distilbert_model,\n",
    "    distilbert_tokenizer,\n",
    "    opt_generator_model,\n",
    "    opt_generator_tokenizer,\n",
    ")\n",
    "\n",
    "# For DistilBERT + GPT-2\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_distilbert_gpt2,\n",
    "    \"distilbert_gpt2\",\n",
    "    os.path.join(OUTPUT_DIR, \"distilbert_gpt2\"),\n",
    "    distilbert_model,\n",
    "    distilbert_tokenizer,\n",
    "    gpt2_model,\n",
    "    gpt2_tokenizer,\n",
    ")\n",
    "\n",
    "# For DistilBERT + OPT\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_distilbert_opt,\n",
    "    \"distilbert_opt\",\n",
    "    os.path.join(OUTPUT_DIR, \"distilbert_opt\"),\n",
    "    distilbert_model,\n",
    "    distilbert_tokenizer,\n",
    "    opt_generator_model,\n",
    "    opt_generator_tokenizer,\n",
    ")\n",
    "\n",
    "# For Electra + OPT-Generator\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_electra_opt_generator,\n",
    "    \"electra_Generator\",\n",
    "    os.path.join(OUTPUT_DIR, \"electra_opt\"),\n",
    "    electra_model,\n",
    "    electra_tokenizer,\n",
    "    opt_generator_model,\n",
    "    opt_generator_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# For Electra + GPT-2\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_electra_gpt2,\n",
    "    \"electra_gpt2\",\n",
    "    os.path.join(OUTPUT_DIR, \"electra_gpt2\"),\n",
    "    electra_model,\n",
    "    electra_tokenizer,\n",
    "    gpt2_model,\n",
    "    gpt2_tokenizer,\n",
    ")\n",
    "\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,  \n",
    "    selected_test_labels,  \n",
    "    detect_with_opt_classifier_opt_generator,  \n",
    "    \"opt_classifier_opt_generator\",  \n",
    "    os.path.join(OUTPUT_DIR, \"opt_classifier_opt_generator\"),  \n",
    "    opt_classifier_model,  \n",
    "    opt_classifier_tokenizer, \n",
    "    opt_generator_model,  \n",
    "    opt_generator_tokenizer  \n",
    ")\n",
    "\n",
    "# For OPT-Classifier + GPT-2\n",
    "evaluate_and_save_results(\n",
    "    selected_test_texts,\n",
    "    selected_test_labels,\n",
    "    detect_with_opt_classifier_gpt2,\n",
    "    \"opt_classifier_gpt2\",\n",
    "    os.path.join(OUTPUT_DIR, \"opt_classifier_gpt2\"),\n",
    "    opt_classifier_model,\n",
    "    opt_classifier_tokenizer,\n",
    "    gpt2_model,\n",
    "    gpt2_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733e735-7d87-4e45-a3e2-0802618d2f28",
   "metadata": {
    "id": "44024dbf-a49d-4901-8d34-cab79c43429c"
   },
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Evaluate Individual Models</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>\n",
    "    This code block evaluates the performance of individual models (DistilBERT, OPT, and ELECTRA) for fake news detection on a balanced sample of test data. \n",
    "    The evaluation calculates metrics such as accuracy, F1-score, and ROC-AUC, generates confusion matrices and performance curves, and saves the results \n",
    "    to structured output directories for further analysis.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.kaggle.com/code/norhanahmed34/deberta-tensorflow-final-2</li>\n",
    "    <li>https://www.kaggle.com/code/evilspirit05/bert-based-fake-news-detector/notebook</li>\n",
    "    <li>https://github.com/Navy10021/FakeLense/blob/main/code/model_code.py</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Steve Mwika + Mallikarjuna Reddy Bobbala</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/18/2024 - major change made for this submission</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f9cba-98c2-4c51-9a4f-488c07be834e",
   "metadata": {
    "id": "2b3f9cba-98c2-4c51-9a4f-488c07be834e"
   },
   "outputs": [],
   "source": [
    "os.makedirs('./outputs/distilbert_only', exist_ok=True)\n",
    "\n",
    "# Function to sample a subset of test data\n",
    "def sample_test_data(test_texts, test_labels, fraction=0.1, random_state=42):\n",
    "    test_data = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "    sampled_data = test_data.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=fraction, random_state=random_state)\n",
    "    )\n",
    "    # Print the number of test cases sampled\n",
    "    total_samples = len(sampled_data)\n",
    "    print(f\"Sampled {total_samples} test cases from the test dataset.\")\n",
    "    print(f\"Distribution of sampled labels:\\n{sampled_data['label'].value_counts()}\")\n",
    "\n",
    "    return sampled_data['text'].tolist(), sampled_data['label'].tolist()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model_name, model, tokenizer, test_texts, test_labels, output_dir):\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Detection function\n",
    "    def detect(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        fake_prob = probs[0][0].item()\n",
    "        real_prob = probs[0][1].item()\n",
    "        prediction = 1 if real_prob >= 0.5 else 0\n",
    "        return prediction, real_prob\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_true = test_labels\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "\n",
    "    for text in test_texts:\n",
    "        pred_label, real_prob = detect(text)\n",
    "        y_pred.append(pred_label)\n",
    "        y_score.append(real_prob)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    log_loss_value = log_loss(y_true, y_score)\n",
    "    brier_score = brier_score_loss(y_true, y_score)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=['Fake', 'Real'])\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_log = {\n",
    "        \"Log Loss\": log_loss_value,\n",
    "        \"Brier Score Loss\": brier_score,\n",
    "        \"Matthews Correlation Coefficient\": mcc,\n",
    "        \"Cohen's Kappa Score\": kappa,\n",
    "        \"ROC AUC Score\": roc_auc,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "    # Save metrics to log file\n",
    "    with open(os.path.join(output_dir, f\"{model_name}_metrics_log.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "        for metric, value in metrics_log.items():\n",
    "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame(list(metrics_log.items()), columns=[\"Metric\", \"Score\"])\n",
    "    metrics_df.to_csv(os.path.join(output_dir, f\"{model_name}_metrics_summary.csv\"), index=False)\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])\n",
    "    cm_display.plot(cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot and save ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "    roc_display.plot()\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot and save Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    pr_display = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "    pr_display.plot()\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_precision_recall_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot metric overview\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(metrics_log.keys(), metrics_log.values(), color='skyblue')\n",
    "    plt.title(f\"Metrics Overview - {model_name}\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_metrics_overview.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Evaluation complete for {model_name}. Metrics, logs, and plots saved to: {output_dir}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample the test data\n",
    "sampled_test_texts, sampled_test_labels = sample_test_data(test_texts, test_labels, fraction=0.1)\n",
    "\n",
    "# Models to Evaluate\n",
    "MODELS = {\n",
    "    \"DistilBERT\": load_distilbert,\n",
    "    \"OPT\": load_opt_classifier,\n",
    "    \"ELECTRA\": load_electra\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "for model_name, loader_function in MODELS.items():\n",
    "    model, tokenizer = loader_function()\n",
    "    model.to(device)\n",
    "    evaluate_model(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        test_texts=sampled_test_texts,\n",
    "        test_labels=sampled_test_labels,\n",
    "        output_dir=os.path.join(OUTPUT_DIR, f\"{model_name.lower()}_only\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefa2fb-21ea-43b6-a306-cee2eae018f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5d581-5ce7-45c6-a599-ce0db976911f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
